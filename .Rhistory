gradient_log_likelihood <- function(y, X, beta, noise_var = 1) {
# Calculate the gradient of the log-likelihood with respect to coefficients 'beta'.
n <- length(y)
mu <- linear_model(X, beta)
residuals <- y - mu
gradient <- t(X) %*% residuals / noise_var
return(gradient)
}
numerical_gradient <- function(y, X, beta, noise_var = 1, epsilon = 1e-6) {
numerical_gradient <- function(y, X, beta, noise_var = 1, epsilon = 1e-6) {
# Numerically calculate the gradient of the log-likelihood for testing purposes.
gradient_num <- numeric(length(beta))
for (i in seq_along(beta)) {
beta_plus_epsilon <- beta
beta_plus_epsilon[i] <- beta_plus_epsilon[i] + epsilon
ll_plus <- log_likelihood(y, X, beta_plus_epsilon, noise_var)
beta_minus_epsilon <- beta
beta_minus_epsilon[i] <- beta_minus_epsilon[i] - epsilon
ll_minus <- log_likelihood(y, X, beta_minus_epsilon, noise_var)
gradient_num[i] <- (ll_plus - ll_minus) / (2 * epsilon)
}
return(gradient_num)
}
# Calculate gradient using the implemented function
gradient <- gradient_log_likelihood(y_test, X_test, beta_initial)
log_likelihood <- function(y, X, beta, noise_var = 1) {
# Calculate log-likelihood of the linear model given the true labels 'y',
# input features 'X', coefficients 'beta', and residual variance 'noise_var'.
n <- length(y)
mu <- X %*% beta
ll <- -0.5 * n * (log(2 * pi * noise_var) + sum((y - mu)^2) / noise_var)
return(ll)
}
gradient_log_likelihood <- function(y, X, beta, noise_var = 1) {
# Calculate the gradient of the log-likelihood with respect to coefficients 'beta'.
n <- length(y)
mu <- X %*% beta
residuals <- y - mu
gradient <- t(X) %*% residuals / noise_var
return(gradient)
}
# Calculate gradient using the implemented function
gradient <- gradient_log_likelihood(y_test, X_test, beta_initial)
# Calculate numerical gradient for testing
gradient_num <- numerical_gradient(y_test, X_test, beta_initial)
gradient
gradient_num
gradient = function(y, X, beta, noise_variance = 1){
n = length(y)
mu = X %*% beta
grad = t(X) %*% (y - mu)/noise_variance
-diag(1/noise_variance, length(y), length(y)) %*% (y - X%*%beta)
return(grad)
}
log_likelihood(y = y_test, X = X_test, beta = beta_initial)
gradient(y = y_test, X = X_test, beta = beta_initial)
log_likelihood = function(beta, y, X, noise_variance = 1){
n = length(y)
mu = X %*% beta
ll = -1/2 * n *log(2*pi*noise_variance) -1/2*sum((y - mu)^2/noise_variance)
#-length(y)/2*log(2*pi)- 1/2*log(det(diag(noise_variance, nrow = length(y), ncol = length(y)))) - 1/2*t(y - X%*%beta)%*%diag(1/noise_variance, nrow = length(y), ncol = length(y))%*%(y - X%*%beta)
return(ll)
}
gradient = function(y, X, beta, noise_variance = 1){
n = length(y)
mu = X %*% beta
grad = t(X) %*% (y - mu)/noise_variance
-diag(1/noise_variance, length(y), length(y)) %*% (y - X%*%beta)
return(grad)
}
log_likelihood(y = y_test, X = X_test, beta = beta_initial)
gradient(y = y_test, X = X_test, beta = beta_initial)
approx_grad_beta <- function(func, beta, ..., dx = .Machine$double.eps^(1/3)) {
numerical_grad <- rep(0, length(beta))
fn = function(beta) func(beta, ...)
for(i in 1:length(beta)){
e = rep(0, times = length(beta))
e[i] = 1
numerical_grad[i] = (fn(beta + dx*e) - fn(beta - dx*e))/(2*dx)
}
return(numerical_grad)
}
approx_grad_beta(log_likelihood, beta = init, X = X, y = y)
gradient(y = y, X = X, beta = init)
find_mle_BFGS = stats::optim(par = init, fn = log_likelihood, gr = gradient, X = X, y = y, beta = beta_hat, method = "BFGS")
gradient(y = y, X = X, beta = init)
log_likelihood(y = y_test, X = X_test, beta = beta_initial)
approx_grad_beta <- function(func, beta, ..., dx = .Machine$double.eps^(1/3)) {
numerical_grad <- rep(0, length(beta))
fn = function(beta) func(beta, ...)
for(i in 1:length(beta)){
e = rep(0, times = length(beta))
e[i] = 1
numerical_grad[i] = (fn(beta + dx*e) - fn(beta - dx*e))/(2*dx)
}
return(numerical_grad)
}
log_likelihood = function(beta, y, X, noise_variance = 1){
n = length(y)
mu = X %*% beta
ll = -1/2 * n *log(2*pi*noise_variance) -1/2*sum((y - mu)^2/noise_variance)
#-length(y)/2*log(2*pi)- 1/2*log(det(diag(noise_variance, nrow = length(y), ncol = length(y)))) - 1/2*t(y - X%*%beta)%*%diag(1/noise_variance, nrow = length(y), ncol = length(y))%*%(y - X%*%beta)
return(ll)
}
log_likelihood(beta_hat, y, X)
find_mle_BFGS = stats::optim(par = init, fn = log_likelihood, gr = gradient, X = X, y = y, beta = beta_hat, method = "BFGS")
log_likelihood
log_likelihood(beta = init, X = X, y = y)
approx_grad_beta(log_likelihood, beta = init, X = X, y = y)
gradient(y = y, X = X, beta = init)
find_mle_BFGS = stats::optim(par = init, fn = log_likelihood, gr = gradient, X = X, y = y, method = "BFGS")
find_mle_BFGS$par
find_mle_BFGS = stats::optim(par = init, fn = log_likelihood, gr = gradient, control(fnscale = -1), X = X, y = y, method = "BFGS")
find_mle_BFGS = stats::optim(par = init, fn = log_likelihood, gr = gradient, control = list(fnscale = -1), X = X, y = y, method = "BFGS")
find_mle_BFGS$par
find_mle_pseudo_inv(X, y)
log_likelihood = function(beta, y, X, noise_variance = 1){
n = length(y)
mu = X %*% beta
ll = -1/2 * n *log(2*pi*noise_variance) -1/2*sum((y - mu)^2/noise_variance)
return(ll)
}
gradient = function(y, X, beta, noise_variance = 1){
n = length(y)
mu = X %*% beta
grad = t(X) %*% (y - mu)/noise_variance
return(grad)
}
approx_grad_beta <- function(func, beta, ..., dx = .Machine$double.eps^(1/3)) {
numerical_grad <- rep(0, length(beta))
fn = function(beta) func(beta, ...)
for(i in 1:length(beta)){
e = rep(0, times = length(beta))
e[i] = 1
numerical_grad[i] = (fn(beta + dx*e) - fn(beta - dx*e))/(2*dx)
}
return(numerical_grad)
}
init = rep(0, 5)
log_likelihood(beta = init, X = X, y = y)
approx_grad_beta(log_likelihood, beta = init, X = X, y = y)
gradient(y = y, X = X, beta = init)
find_mle_BFGS = stats::optim(par = init, fn = log_likelihood, gr = gradient, control = list(fnscale = -1), X = X, y = y, method = "BFGS")
find_mle_BFGS$par
find_mle_pseudo_inv(X, y)
gradient_num(log_likelihood, beta = init, X = X, y = y)
gradient_num <- function(func, beta, ..., dx = .Machine$double.eps^(1/3)) {
numerical_grad <- rep(0, length(beta))
fn = function(beta) func(beta, ...)
for(i in 1:length(beta)){
e = rep(0, times = length(beta))
e[i] = 1
numerical_grad[i] = (fn(beta + dx*e) - fn(beta - dx*e))/(2*dx)
}
return(numerical_grad)
}
test_that("observed gradient is close to numerical gradient", {
df = simulate_data(100, 5)
X = df$design
y = df$outcome
beta_hat = df$coef_true
grad = gradient(beta = beta_hat, X = X, y = y)
gradient_num = approx_grad(log_likelihood, X = X, y = y, beta = beta_hat)
result = are_all_close(grad, gradient_num)
expect_true(result)
})
approx_grad() <- function(func, beta, ..., dx = .Machine$double.eps^(1/3)) {
numerical_grad <- rep(0, length(beta))
fn = function(beta) func(beta, ...)
for(i in 1:length(beta)){
e = rep(0, times = length(beta))
e[i] = 1
numerical_grad[i] = (fn(beta + dx*e) - fn(beta - dx*e))/(2*dx)
}
return(numerical_grad)
}
approx_grad <- function(func, beta, ..., dx = .Machine$double.eps^(1/3)) {
numerical_grad <- rep(0, length(beta))
fn = function(beta) func(beta, ...)
for(i in 1:length(beta)){
e = rep(0, times = length(beta))
e[i] = 1
numerical_grad[i] = (fn(beta + dx*e) - fn(beta - dx*e))/(2*dx)
}
return(numerical_grad)
}
test_that("observed gradient is close to numerical gradient", {
df = simulate_data(100, 5)
X = df$design
y = df$outcome
beta_hat = df$coef_true
grad = gradient(beta = beta_hat, X = X, y = y)
gradient_num = approx_grad(log_likelihood, X = X, y = y, beta = beta_hat)
result = are_all_close(grad, gradient_num)
expect_true(result)
})
test_that("linalg and optim least-sq coincide", {
df = simulate_data(100, 5)
X = df$design
y = df$outcome
beta_hat = df$coef_true
linalg_result = find_mle_pseudo_inv(X, y)
init = rep(0, length(beta_hat))
optim_result = stats::optim(par = init, fn = log_likelihood, gr = gradient, X = X, y = y, method = "BFGS")$par
result = are_all_close(linalg_result, optim_result)
expect_true(result)
})
df = simulate_data(100, 5)
X = df$design
y = df$outcome
beta_hat = df$coef_true
df = simulate_data(100, 5)
View(df)
X = df$design
y = df$outcome
beta_hat = df$coef_true
linalg_result = find_mle_pseudo_inv(X, y)
init = rep(0, length(beta_hat))
optim_result = stats::optim(par = init, fn = log_likelihood, gr = gradient, control = list(fnscale = -1), X = X, y = y, method = "BFGS")$par
result = are_all_close(linalg_result, optim_result)
test_that("linalg and optim least-sq coincide", {
df = simulate_data(100, 5)
X = df$design
y = df$outcome
beta_hat = df$coef_true
linalg_result = find_mle_pseudo_inv(X, y)
init = rep(0, length(beta_hat))
optim_result = stats::optim(par = init, fn = log_likelihood, gr = gradient, control = list(fnscale = -1), X = X, y = y, method = "BFGS")$par
result = are_all_close(linalg_result, optim_result)
expect_true(result)
})
#Find MLE using BFGS
find_mle_BFGS = function(X, y, p) {
log_likelihood = function(beta, y, X, noise_variance = 1){
n = length(y)
mu = X %*% beta
ll = -1/2 * n *log(2*pi*noise_variance) -1/2*sum((y - mu)^2/noise_variance)
return(ll)
}
gradient = function(y, X, beta, noise_variance = 1){
n = length(y)
mu = X %*% beta
grad = t(X) %*% (y - mu)/noise_variance
return(grad)
}
approx_grad <- function(func, beta, ..., dx = .Machine$double.eps^(1/3)) {
numerical_grad <- rep(0, length(beta))
fn = function(beta) func(beta, ...)
for(i in 1:length(beta)){
e = rep(0, times = length(beta))
e[i] = 1
numerical_grad[i] = (fn(beta + dx*e) - fn(beta - dx*e))/(2*dx)
}
return(numerical_grad)
}
init = rep(0, p)
if(are_all_close(gradient(y = y, X = X, beta = init), approx_grad(log_likelihood, beta = init, y = y, X = X)) == F){
stop("Calculated Gradient and Numerical Gradient do not match")
}
mle = stats::optim(par = init, fn = log_likelihood, gr = gradient, control = list(fnscale = -1), X = X, y = y, method = "BFGS")
return(mle)
}
test_that("observed gradient is close to numerical gradient", {
df = simulate_data(100, 5)
X = df$design
y = df$outcome
beta_hat = df$coef_true
grad = gradient(beta = beta_hat, X = X, y = y)
gradient_num = approx_grad(log_likelihood, X = X, y = y, beta = beta_hat)
result = are_all_close(grad, gradient_num)
expect_true(result)
})
test_that("linalg and optim least-sq coincide", {
df = simulate_data(100, 5)
X = df$design
y = df$outcome
beta_hat = df$coef_true
linalg_result = find_mle_pseudo_inv(X, y)
BFGS_result = find_mle_BFGS()
init = rep(0, length(beta_hat))
optim_result = stats::optim(par = init, fn = log_likelihood, gr = gradient, control = list(fnscale = -1), X = X, y = y, method = "BFGS")$par
result = are_all_close(linalg_result, optim_result)
expect_true(result)
})
gradient = function(y, X, beta, noise_variance = 1){
n = length(y)
mu = X %*% beta
grad = t(X) %*% (y - mu)/noise_variance
return(grad)
}
#Find MLE using BFGS
find_mle_BFGS = function(X, y, p) {
log_likelihood = function(beta, y, X, noise_variance = 1){
n = length(y)
mu = X %*% beta
ll = -1/2 * n *log(2*pi*noise_variance) -1/2*sum((y - mu)^2/noise_variance)
return(ll)
}
gradient = function(y, X, beta, noise_variance = 1){
n = length(y)
mu = X %*% beta
grad = t(X) %*% (y - mu)/noise_variance
return(grad)
}
approx_grad <- function(func, beta, ..., dx = .Machine$double.eps^(1/3)) {
numerical_grad <- rep(0, length(beta))
fn = function(beta) func(beta, ...)
for(i in 1:length(beta)){
e = rep(0, times = length(beta))
e[i] = 1
numerical_grad[i] = (fn(beta + dx*e) - fn(beta - dx*e))/(2*dx)
}
return(numerical_grad)
}
init = rep(0, p)
mle = stats::optim(par = init, fn = log_likelihood, gr = gradient, control = list(fnscale = -1), X = X, y = y, method = "BFGS")
return(mle)
}
test_that("linalg and optim least-sq coincide", {
df = simulate_data(100, 5)
X = df$design
y = df$outcome
beta_hat = df$coef_true
linalg_result = find_mle_pseudo_inv(X, y)
BFGS_result = find_mle_BFGS()
init = rep(0, length(beta_hat))
optim_result = stats::optim(par = init, fn = log_likelihood, gr = gradient, control = list(fnscale = -1), X = X, y = y, method = "BFGS")$par
result = are_all_close(linalg_result, optim_result)
expect_true(result)
})
#Find MLE using BFGS
find_mle_BFGS = function(X, y, p) {
log_likelihood = function(beta, y, X, noise_variance = 1){
n = length(y)
mu = X %*% beta
ll = -1/2 * n *log(2*pi*noise_variance) -1/2*sum((y - mu)^2/noise_variance)
return(ll)
}
gradient = function(y, X, beta, noise_variance = 1){
n = length(y)
mu = X %*% beta
grad = t(X) %*% (y - mu)/noise_variance
return(grad)
}
approx_grad <- function(func, beta, ..., dx = .Machine$double.eps^(1/3)) {
numerical_grad <- rep(0, length(beta))
fn = function(beta) func(beta, ...)
for(i in 1:length(beta)){
e = rep(0, times = length(beta))
e[i] = 1
numerical_grad[i] = (fn(beta + dx*e) - fn(beta - dx*e))/(2*dx)
}
return(numerical_grad)
}
init = rep(0, p)
if(are_all_close(gradient(y = y, X = X, beta = init), approx_grad(log_likelihood, beta = init, y = y, X = X)) == F){
stop("Calculated Gradient and Numerical Gradient do not match")
}
mle = stats::optim(par = init, fn = log_likelihood, gr = gradient, control = list(fnscale = -1), X = X, y = y, method = "BFGS")
return(mle)
}
test_that("linalg and optim least-sq coincide", {
df = simulate_data(100, 5)
X = df$design
y = df$outcome
beta_hat = df$coef_true
linalg_result = find_mle_pseudo_inv(X, y)
BFGS_result = find_mle_BFGS(X, y, length(beta_hat))
init = rep(0, length(beta_hat))
optim_result = stats::optim(par = init, fn = log_likelihood, gr = gradient, control = list(fnscale = -1), X = X, y = y, method = "BFGS")$par
result = are_all_close(linalg_result, optim_result)
expect_true(result)
})
library(devtools)
devtools::test()
source("C:/Users/samfa/hiperglm/tests/testthat/helper.R")
devtools::test()
devtools::test()
devtools::test()
df = simulate_data(100, 5)
X = df$design
y = df$outcome
beta_hat = df$coef_true
df = simulate_data(100, 5)
X = df$design
y = df$outcome
beta_hat = df$coef_true
linalg_result = find_mle_pseudo_inv(X, y)
BFGS_result = find_mle_BFGS(X, y, length(beta_hat))
View(BFGS_result)
BFGS_result = find_mle_BFGS(X, y, length(beta_hat))$par
linalg_result = find_mle_pseudo_inv(X, y)
result = are_all_close(linalg_result, BFGS_result)
test_that("linalg and optim least-sq coincide", {
df = simulate_data(100, 5)
X = df$design
y = df$outcome
beta_hat = df$coef_true
linalg_result = find_mle_pseudo_inv(X, y)
BFGS_result = find_mle_BFGS(X, y, length(beta_hat))$par
result = are_all_close(linalg_result, BFGS_result)
expect_true(result)
})
devtools::test()
test_that("linalg and optim least-sq coincide", {
df = simulate_data(100, 5)
X = df$design
y = df$outcome
linalg_result = find_mle_pseudo_inv(X, y)
BFGS_result = find_mle_BFGS(X, y, ncol(X))$par
result = are_all_close(linalg_result, BFGS_result)
expect_true(result)
})
test_that("linalg and optim least-sq coincide", {
df = simulate_data(100, 5)
X = df$design
y = df$outcome
p = ncol(X)
linalg_result = find_mle_pseudo_inv(X, y)
BFGS_result = find_mle_BFGS(X, y, p)$par
result = are_all_close(linalg_result, BFGS_result)
expect_true(result)
})
test_that("observed gradient is close to numerical gradient", {
df = simulate_data(100, 5)
X = df$design
y = df$outcome
p = ncol(X)
init = rep(0, p)
grad = gradient(beta = init, X = X, y = y)
gradient_num = approx_grad(log_likelihood, X = X, y = y, beta = init)
result = are_all_close(grad, gradient_num)
expect_true(result)
})
devtools::test()
test_that("linalg and optim least-sq coincide", {
df = simulate_data(100, 5)
X = df$design
y = df$outcome
p = ncol(X)
linalg_result = find_mle_pseudo_inv(X, y)
BFGS_result = find_mle_BFGS(X, y, p)$par
result = are_all_close(linalg_result, BFGS_result)
expect_true(result)
})
testthat::expect_true(are_all_close(
analytical_grad, numerical_grad, abs_tol = Inf, rel_tol = 1e-3
))
library(devtools)
devtools::test()
test_that("TRUE when <abs_tol, <rel_tol", {
w = c(6 + 1e-7, 5 + 1e-7, 4 + 1e-7)
v = c(6, 5, 4)
abs_diff <- abs(v - w)
all(abs_diff < 1e-6) #TRUE
all(abs_diff < 1e-6 * pmax(abs(v), abs(w))) #TRUE
result <- are_all_close(v, w)
expect_true(result)
})
are_all_close <- function(v, w, abs_tol = 1e-6, rel_tol = 1e-6) {
abs_diff <- abs(v - w)
are_all_within_atol <- all(abs_diff < abs_tol)
are_all_within_rtol <- all(abs_diff < rel_tol * pmax(abs(v), abs(w)))
return(are_all_within_atol && are_all_within_rtol)
}
simulate_data <- function(
n_obs, n_pred, model = "linear", intercept = NULL,
coef_true = NULL, design = NULL, seed = NULL, signal_to_noise = 0.1
) {
if (!is.null(seed)) {
set.seed(seed)
}
if (is.null(coef_true)) {
coef_true <- rnorm(n_pred, sd = 1 / sqrt(n_pred))
}
if (is.null(design)) {
design <- matrix(rnorm(n_obs * n_pred), nrow = n_obs, ncol = n_pred)
}
if (!is.null(intercept)) {
if (!is.numeric(intercept)) {
stop("The intercept argument must be numeric.")
}
coef_true <- c(intercept, coef_true)
design <- cbind(rep(1, n_obs), design)
}
expected_mean <- as.vector(design %*% coef_true)
noise_magnitude <- sqrt(var(expected_mean) / signal_to_noise^2)
noise <- noise_magnitude * rnorm(n_obs)
outcome <- expected_mean + noise
return(list(design = design, outcome = outcome, coef_true = coef_true))
}
source("C:/Users/samfa/hiperglm/R/find_mle.R")
test_that("TRUE when <abs_tol, <rel_tol", {
w = c(6 + 1e-7, 5 + 1e-7, 4 + 1e-7)
v = c(6, 5, 4)
abs_diff <- abs(v - w)
all(abs_diff < 1e-6) #TRUE
all(abs_diff < 1e-6 * pmax(abs(v), abs(w))) #TRUE
result <- are_all_close(v, w)
expect_true(result)
})
test_that("TRUE when <abs_tol, <rel_tol", {
w = c(6 + 1e-7, 5 + 1e-7, 4 + 1e-7)
v = c(6, 5, 4)
abs_diff <- abs(v - w)
all(abs_diff < 1e-6) #TRUE
all(abs_diff < 1e-6 * pmax(abs(v), abs(w))) #TRUE
result <- are_all_close(v, w)
expect_true(result)
})
devtools::test()
devtools::test()
